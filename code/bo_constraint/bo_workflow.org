-*- mode: org -*-
:PROPERTIES:
:header-args: :session bo-workflow-default-session :async t
:END:

#+TITLE: Bayesian Optimization basic workflow
#+AUTHOR: minhvu
#+DATE: 20190607
#+STARTUP: inlineimages


* Introduction
Note: Highlight that we can apply BO with constraint-score for any parametric DR methods, not only t-SNE.

** Problem of finding a good visualization with t-SNE
** Motivation of using user constraints to evaluate the visualization
** Proposed method

* Background
** Baysian Optimization
** Visualization quality metric
** Semantic clustering
Note: Can add CIFAR10 dataset and show another way to group the points in the visualization (i.e., not by classes: dog, cat, plane, but by `semantic`: blue-sky, green-grass, ...).

* Related Works

* Proposed Method

** Introduction to constraint preserving score

** Other proposition for constraint score

*** Constrastive pair

#+BEGIN_SRC ipython :results silent
from common.dataset import constraint
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer
import math

contrastive_constraints = constraint.generate_contrastive_constraints(labels=y, n_links=100)

def contrastive_score(y, eps=1e-32):
    y = Normalizer().fit_transform(y)
    
    score = 0.0
    for constraint in contrastive_constraints:
        fx, fx_positive, fx_negative = y[constraint]
        numerator =  math.exp(np.dot(fx, fx_positive))
        denominator = math.exp(np.dot(fx, fx_positive)) + math.exp(np.dot(fx, fx_negative)) + eps
        score += math.log(numerator / denominator)

    return score / len(contrastive_constraints)
#+END_SRC


*** Triplet

*** Cosine similarity
+ Take two point \( y_i, y_j \), calculate \( \textnormal{cosine_similarity}(y_i, y_j) \).
+ If \( y_i, y_j \) are similar, their cosine_similarity must close to 1, so the larger the better.
+ If \( y_i, y_j \) are dissimilar, ther cosine_similarity must close to -1, so the smaller the better.
+ The constraint score is thus similar_score + (-dissimilar_score).

#+BEGIN_SRC ipython :results silent
from scipy.spatial.distance import cosine

def cosine_score(y, sim_links, dis_links):
    sim_score = (0.0 if 0 == len(sim_links)
                 else sum([1 - cosine(y[i], y[j]) for i, j, _ in sim_links]) / len(sim_links))
    dis_score = (0.0 if 0 == len(dis_links)
                 else sum([1 - cosine(y[i], y[j]) for i, j, _ in dis_links]) / len(dis_links))
    return sim_score, dis_score, sim_score + (-dis_score)    
#+END_SRC


*** Cosin distance ratio

#+BEGIN_SRC ipython :results silent
from scipy.spatial.distance import cosine
from scipy.special import expit as sigmoid
import math

def cosine_ratio_score(x, y, sim_links, dis_links, eps=1e-32):
    sim_score = 0.0
    dis_score = 0.0
    
    for i, j, link_type in np.vstack([sim_links, dis_links]):
        y_sim = 1 - cosine(y[i], y[j]) + eps
        x_sim = 1 - cosine(x[i], x[j]) + eps
        ratio_cap_in_range = y_sim / x_sim # testing no cap

        if link_type in ["sim_link", 1]:
            sim_score += ratio_cap_in_range
        elif link_type in ["dis_link", -1]:
            dis_score += ratio_cap_in_range

    sim_score = 0.0 if len(sim_links) == 0 else sim_score / len(sim_links)
    dis_score = 0.0 if len(dis_links) == 0 else dis_score / len(dis_links)
        
    return sim_score, dis_score, 0.5 * (sim_score + dis_score)    
#+END_SRC

#+BEGIN_SRC ipython :results silent
def debug_cosine_ratio_score(x, y, sim_links, dis_links):
    for i, j, _ in [sim_links[0]] + [dis_links[0]]:
        y_sim = 1 - cosine(y[i], y[j])
        x_sim = 1 - cosine(x[i], x[j])

        print(i, y[i])
        print(j, y[j])
        print(y_sim, sigmoid(y_sim))
        print(x_sim, sigmoid(x_sim))
        print(y_sim / x_sim, sigmoid(y_sim/x_sim), np.tanh(y_sim/x_sim))
        print(x_sim / y_sim, sigmoid(x_sim/y_sim), np.tanh(x_sim/y_sim))
              
#+END_SRC


** Idea of using constraint score as target function for BO method

* Evaluation

#+BEGIN_SRC ipython :results silent
# setup ipython autoreload and inline plot
%load_ext autoreload
%autoreload 2
%matplotlib inline
#+END_SRC


** Workflow
+ Goal: find the best perplexity using BO method with a target of maximizing the /user constraint preserving score/.
+ It should make clear that the goal of BO method is to find the maximum of a /true target function/, not approximate it. In fact we can observe the *predicted target function* produced by BO method and compare it with the *true target function*.
+ To see how the BO method work in action, we will show the *true target function* (e.g., the constraint scores w.r.t the perplexity) and the *predicted target function* which is the mean function of a Gaussian process model using in BO method.

#+BEGIN_SRC ipython :results silent
import joblib
import datetime

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from matplotlib import gridspec

from bayes_opt import BayesianOptimization
from bayes_opt import UtilityFunction

from common.dataset import dataset
from common.metric.dr_metrics import DRMetric
from common.dataset import constraint

from MulticoreTSNE import MulticoreTSNE
#+END_SRC


** Dataset
+ A subset of the Fashion-MNIST dataset (200 grayscale images of size 28x28). The data are standardized.
+ Normally, we expect to see label-based groups, i.e., the clothes of the same type are places close together. However, the user can give the feedbacks to form *semantic groups*, i.e., trouser + long dress, sandal + ankle boot + sneaker, coat + T-shirt,...

#+BEGIN_SRC ipython
dataset.set_data_home("./data")
dataset_name = "FASHION500"
_, X, y = dataset.load_dataset(dataset_name)
print(X.shape, y.shape)
#+END_SRC

#+RESULTS:
:results:
# Out [37]: 
# output
(500, 784) (500,)

:end:



Prepare two lists of similar and dissimilar pairwise constraints.

#+BEGIN_SRC ipython :results silent
sim_links = constraint.gen_similar_links(labels=y, n_links=50, include_link_type=True)
dis_links = constraint.gen_dissimilar_links(labels=y, n_links=50, include_link_type=True)
#+END_SRC

#+BEGIN_SRC ipython results output
score_name = "contrastive" # ["qij", "cosine", "cosine_ratio", "contrastive"]
constraint_proportion = 1.0
print("Constraint score proportion: ", constraint_proportion, score_name)
#+END_SRC

#+RESULTS:
:results:
# Out [33]: 
# output
Constraint score proportion:  1.0 contrastive

:end:


** Comparison between user constraint preserving score and metric score

Perplexity value range:
#+BEGIN_SRC ipython :results silent
perp_range = np.array(list(range(2, X.shape[0] // 3)))
print(perp_range.shape)
#+END_SRC

Construct the target function, that is a combination of constraint-preserving score and quality metric score (John's metric).
#+BEGIN_SRC ipython :results silent
def tsne_with_metric_and_constraint(perp, debug=False):
    tsne = MulticoreTSNE(perplexity=perp, n_iter=1000, random_state=2019, n_jobs=3,
                         n_iter_without_progress=1000, min_grad_norm=1e-32, eval_interval=20,
                         verbose=debug)

    Z = tsne.fit_transform(X)
    losses = tsne.progress_errors_
    losses = losses[np.where( (0.0 < losses) & (losses < 2.0) )]

    auc_rnx = DRMetric(X, Z).auc_rnx()

    # Q = data_filter._compute_Q(Z)
    # s_sim, s_dis = data_filter.constraint_score(Q, sim_links, dis_links, debug=False)
    # s_links = 0.5 * s_sim + 0.5 * s_dis
    # s_sim, s_dis, s_links = cosine_ratio_score(X, Z, sim_links, dis_links)
    s_links = contrastive_score(Z)
    
    if debug:
        plt.figure(figsize=(12, 10))
        gs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])
        scatter_ax = plt.subplot(gs[0])
        loss_ax = plt.subplot(gs[1])
        
        scatter_ax.scatter(Z[:, 0], Z[:, 1], c=y, alpha=0.4, cmap="jet")
        loss_ax.plot(losses)
        
        plt.show()
        print(f"Debug: contrastive_score={s_links}, auc_rnx={auc_rnx}")
        # print(f"Debug: constraint_proportion={constraint_proportion}, link_score=[{s_sim}, {s_dis}, {s_links}], auc_rnx={auc_rnx}")
    
    return constraint_proportion * s_links + (1 - constraint_proportion) * auc_rnx
#+END_SRC


Build the *true target function* (which is unknown in real application) to demostrate how BO can approximate its maximum value.
#+BEGIN_SRC ipython :async t
from constraint_app import data_filter

df_metric = data_filter.get_metrics_df(dataset_name=dataset_name, base_perp=None, earlystop="")
print("Metric scores: ", len(df_metric))
print(df_metric.head())

df_constraint_score, _ = data_filter.get_constraint_scores_df(
    dataset_name=dataset_name,
    base_perp=None,
    earlystop="",
    constraints=np.vstack([sim_links, dis_links]),
    debug=False,
)
print("\n\nConstraint preserving scores: ", len(df_constraint_score))
print(df_constraint_score.head())

df_target = pd.merge(df_metric, df_constraint_score, how="inner", on="perplexity")
print("\n\nMetrics + constraint scores with new target_score column: ", len(df_target))
df_target["target_score"] = (
    constraint_proportion * df_target["score_all_links"]
    + (1 - constraint_proportion) * df_target["auc_rnx"]
)

print(df_target[["auc_rnx", "score_all_links", "target_score"]].head())

true_target_values = df_target.loc[perp_range, "target_score"].values
true_target_values = true_target_values.reshape(-1, 1)
print(true_target_values.shape)
#+END_SRC

#+RESULTS:
:results:
# Out [18]: 
# output
Metric scores:  165
            kl_divergence   auc_rnx       bic
perplexity                                   
1                0.805106  0.310806  1.622642
2                0.851039  0.483611  1.726937
3                0.878397  0.517260  1.794083
4                0.882262  0.514850  1.814241
5                0.859684  0.522340  1.781513


Constraint preserving scores:  165
            score_all_links  score_dissimilar_links  score_similar_links
perplexity                                                              
1                  0.406838               14.763054           -13.949377
2                  1.084810               14.625494           -12.455874
3                  1.073202               14.590280           -12.443877
4                  1.110531               14.502674           -12.281612
5                  1.182109               14.611629           -12.247411


Metrics + constraint scores with new target_score column:  165
             auc_rnx  score_all_links  target_score
perplexity                                         
1           0.310806         0.406838      0.406838
2           0.483611         1.084810      1.084810
3           0.517260         1.073202      1.073202
4           0.514850         1.110531      1.110531
5           0.522340         1.182109      1.182109
(164, 1)

:end:


** Experiment with BO method

Util function for ploting the decision of BO method at each step
#+BEGIN_SRC ipython :results silent
def posterior(optimizer, x_obs, y_obs, grid):
    optimizer._gp.fit(x_obs, y_obs)

    mu, sigma = optimizer._gp.predict(grid, return_std=True)
    return mu, sigma


def plot_gp(optimizer, x, y, util_func="ucb", kappa=5, xi=0.01):
    fig = plt.figure(figsize=(14, 8))
    steps = len(optimizer.space)
    #     fig.suptitle(
    #         'Gaussian Process and Utility Function After {} Steps'.format(steps),
    #         fontdict={'size':35}
    #     )

    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])
    axis = plt.subplot(gs[0])
    acq = plt.subplot(gs[1])

    x_obs = np.array([[res["params"]["perp"]] for res in optimizer.res])
    y_obs = np.array([res["target"] for res in optimizer.res])

    current_max_target_function = optimizer.max["target"]
    
    mu, sigma = posterior(optimizer, x_obs, y_obs, x)
    # axis.plot(x, y, linewidth=3, label="Target")
    axis.plot(x_obs.flatten(), y_obs, "D", markersize=8, label="Observations", color="r")
    axis.plot(x, mu, "--", color="k", label="Prediction")

    axis.fill(
        np.concatenate([x, x[::-1]]),
        np.concatenate([mu - 1.9600 * sigma, (mu + 1.9600 * sigma)[::-1]]),
        alpha=0.4,
        fc="c",
        ec="None",
        label="95% confidence interval",
    )
    
    # axis.set_xlim((x_obs.min(), x_obs.max()))
    # axis.set_ylim((0.85 * y_obs.min(), 1.15 * y_obs.max()))
    axis.set_ylabel("tsne_with_metric_and_constraint", fontdict={"size": 16})
    # axis.set_xlabel("perplexity", fontdict={"size": 16})

    utility_function = UtilityFunction(kind=util_func, kappa=kappa, xi=xi)
    utility = utility_function.utility(x, optimizer._gp, y_max=current_max_target_function)
    
    acq.plot(x, utility, label=f"Utility Function ({util_func})", color="purple")
    acq.plot(
        x[np.argmax(utility)],
        np.max(utility),
        "*",
        markersize=15,
        label="Next Best Guess",
        markerfacecolor="gold",
        markeredgecolor="k",
        markeredgewidth=1,
    )
    # acq.set_xlim((x_obs.min(), x_obs.max()))
    # acq.set_ylim((0, np.max(utility) + 0.5))
    acq.set_ylabel(f"Utility ({util_func})", fontdict={"size": 16})
    acq.set_xlabel("perplexity", fontdict={"size": 16})

    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.0)
    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.0)

    # debug next best guess
    next_best_guess_param = x[np.argmax(utility)]
    acq.set_title(f"Next best guess param: {next_best_guess_param}", fontdict={"size": 16})

    # draw indicator vline @ the next perplexity
    acq.axvline(next_best_guess_param, color='g', linestyle='--', alpha=0.4)
    axis.axvline(next_best_guess_param, color='g', linestyle='--', alpha=0.4)
    # draw indicator hline @ the current  max value of the  target function
    axis.axhline([current_max_target_function], color='r', linestyle='--', alpha=0.4)

    debug_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    debug_method_name = {
        "ucb": f"ucb_kappa{kappa}",
        "ei": f"ei_xi{xi}",
        "poi": f"poi_xi{xi}"
    }[util_func]

    axis.set_title(f"Figure created @ {debug_time}", size=12)
    plt.suptitle(
        f"GP ({debug_method_name} utility function) after {steps} steps with best predicted perlexity = {optimizer.max['params']['perp']:.2f}",
        size=20,
    )
    plt.savefig(f"./plots/{score_name}/{debug_method_name}_constraint{constraint_proportion}_{dataset_name}_step{steps}.png", bbox_inches="tight")
#+END_SRC

Construct a Bayesian Optimizer, that will take into account the target function that we want to maximize (=tsne_with_metric_and_constraint= in our case) and a space of its parameter =perp=.

Using the default utility function *Upper Confidence Bound (UCB)* which has a free param  \( \kappa \). Set \(\kappa = 5\) to compromise the /exploitation/ and /exploration/.
Start the optimization process with 5 random init points, that means BO will evaluate the target function 5 times with 5 randomly seletecd =perp= params.
Then run the optimization loop some more iterations and plot the decision of GP model of the BO method.


#+BEGIN_SRC ipython :results silent :async t
n_total_runs = 15
n_random_inits = 5
kappa = 5
xi = 0.025 # [0.001, 0.0025, 0.005]
util_func = "ei" # ["ucb", "ei", "poi"]

optimizer = BayesianOptimization(
    tsne_with_metric_and_constraint,
    {"perp": (2, X.shape[0] // 3)},
    random_state=2048,
)

optimizer.maximize(acq=util_func, init_points=n_random_inits, n_iter=0,
                   kappa=kappa, xi=xi)
plot_gp(optimizer, x=perp_range.reshape(-1, 1), y=true_target_values,
        util_func=util_func,
        kappa=kappa, xi=xi)

for i in range(n_total_runs - n_random_inits):
    optimizer.maximize(acq=util_func, init_points=0, n_iter=1,
                       kappa=kappa, xi=xi)
    print("Current max: ", optimizer.max)
    plot_gp(optimizer, x=perp_range.reshape(-1, 1), y=true_target_values,
            util_func=util_func,
            kappa=kappa, xi=xi)

print("Final best perp:" , optimizer.max)
#+END_SRC


** Evaluation of the visualization



* Discussion
** Pros and Cons 
** Conclusion
** Future works

* Dev Notes

*** DONE BUG Evaluation of target function does not match the value of *true target function*
See the following figure after having 5 random evaluated points:
[[./plots/bo_DIGITS_niter6.png]]
(The values of the 'evaluated' target function do not lie in the *real target function*)

+ Still bug (12/06) values when calculating (call function) manually and values getting from =target_df= are slightly different.

- Source of error:: MulticoreTSNE is setup in /early-stop/ fashion -> should set =n_iter_without_progress=1000= *and* =min_grad_norm=1e-32= to disable this feature.
- That will assure that all =n_iter= will be run completely.

#+BEGIN_SRC ipython :async t
# DEBUG the evaluated target function value

# true target function
target_param = 3
print(df_target.loc[target_param])

# evaluated target function value
print(tsne_with_metric_and_constraint(perp=target_param, debug=True))
#+END_SRC

#+RESULTS:
:results:
# Out [70]: 
# output
kl_divergence              0.878397
auc_rnx                    0.517260
bic                        1.794083
score_all_links            1.073202
score_dissimilar_links    14.590280
score_similar_links      -12.443877
target_score               1.073202
Name: 3, dtype: float64

[python]Running modified version:  MODIFIED_WITH_EARLY_STOP
SO file:  /opt/anaconda3/lib/python3.6/site-packages/MulticoreTSNE-0.2-py3.6-linux-x86_64.egg/MulticoreTSNE/libtsne_multicore_minh.so
Debug: contrastive_score=54.65154180463306, auc_rnx=0.5172599166684981
54.65154180463306

# text/plain
: <Figure size 864x720 with 2 Axes>

# image/png
[[file:obipy-resources/0706fd9409e5f8cd7b603830b1d899a5abe2c020/aa5e6aaf3464a8adc8f142eb700307aa888c04ac.png]]
:end:


#+BEGIN_SRC ipython  :async t

#+END_SRC

